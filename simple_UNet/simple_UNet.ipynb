{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215089f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28e346e8890>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from simple_UNet import UNet\n",
    "import os\n",
    "from hyperparameters import *\n",
    "# from loss_function import DiceLoss\n",
    "import time\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8255a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5e55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channels=3,\n",
    "            out_channels=1,\n",
    "            n_class=1,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            stride=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddca6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(size=(3,3,1024,1024), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b79c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = int(train_set_size / batch_size)\n",
    "iterations = epochs * n_iters\n",
    "step_size = 2*n_iters\n",
    "if not os.path.exists('./results'):\n",
    "    os.mkdir('./results')\n",
    "save_PATH = f'./results/{epochs}epochs_{lr}lr_{batch_size}batch'\n",
    "if not os.path.exists(save_PATH):\n",
    "    os.mkdir(save_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8c3c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(opt, step_size=step_size, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b4ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss, valid_loss, accuracy = [], [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "            step = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for x, y in dataloader:\n",
    "                x = torch.permute(x, (0, 3, 2, 1))\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                if phase == 'train':\n",
    "                    # zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    outputs = torch.squeeze(outputs)\n",
    "                    y = y.to(torch.float64)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "\n",
    "                    # the backward pass frees the graph memory, so there is no \n",
    "                    # need for torch.no_grad in this training pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        outputs = torch.squeeze(outputs)\n",
    "                        loss = loss_fn(outputs, y)\n",
    "\n",
    "                # stats - whatever is the phase\n",
    "                acc = acc_fn(outputs, y, batch_size)\n",
    "\n",
    "                running_acc  += acc*dataloader.batch_size\n",
    "                running_loss += loss*dataloader.batch_size \n",
    "\n",
    "                if step % 10 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))\n",
    "                    # print(torch.cuda.memory_summary())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_acc / len(dataloader.dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return train_loss, valid_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5aea829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.7036934812373147  Acc: 0.39170533418655396  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6957467269258985  Acc: 0.4773002564907074  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.7070352376940321  Acc: 0.31131210923194885  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.7005226345828902  Acc: 0.38519516587257385  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.7051764840386567  Acc: 0.26998597383499146  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.7020407322783768  Acc: 0.29310837388038635  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6979523009094919  Acc: 0.359161376953125  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6952284341750499  Acc: 0.4180564880371094  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6955208891478847  Acc: 0.35359877347946167  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.693933338435686  Acc: 0.3967247009277344  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6930694219036001  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.7004 Acc: 0.37033115677973805\n",
      "Current step: 10  Loss: 0.6924254894256592  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6917067766189575  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6911645531654358  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6812 Acc: 0.6514777348499106\n",
      "Epoch 1/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6913093141241461  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6926549777055697  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6869674698130694  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.6884303094788777  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.6815395571316344  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.681131288832313  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6841066643357863  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6876287322235441  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.681749613456202  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6847471216158283  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6866597567596046  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6846 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.6861175298690796  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6778265237808228  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6715706586837769  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6671 Acc: 0.6514777348499106\n",
      "Epoch 2/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6824611863225982  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6922232561239923  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.671160693474269  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.6798656474402265  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.6627294518981216  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.6648213472464249  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6740624357835259  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6830586989927383  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6717960924081467  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6787707870907411  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6830672811373233  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6727 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.6825916171073914  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6678138375282288  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6566634178161621  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6569 Acc: 0.6514777348499106\n",
      "Epoch 3/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6766990536898447  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6936558458353602  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6592427637159176  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.6741540077991204  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.6480135196396077  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.6522012992524424  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6668647604997204  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6806234716618178  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6645477248361544  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6749899085839388  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6813582551331365  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6640 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.6808966398239136  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6603178977966309  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.644790530204773  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6492 Acc: 0.6514777348499106\n",
      "Epoch 4/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6729963552974368  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.696312926599262  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6500876889902202  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.6704672873947629  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.6364756722498897  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.6425129771557294  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6618744402976517  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6797176121575603  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6595074675375827  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6728959775242629  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6810020660102054  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6577 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.680528998374939  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6550675630569458  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6358558535575867  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6437 Acc: 0.6514777348499106\n",
      "Epoch 5/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6709053935492193  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6995774113918969  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6432873002630686  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.6682816244565857  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.627511318368397  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.6350839873386349  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6584578910260461  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6798099596060183  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6559756060393056  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6719013744069571  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6815040176955336  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6532 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.6809988617897034  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.651239812374115  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6287854909896851  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6397 Acc: 0.6514777348499106\n",
      "Epoch 6/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6698478338856149  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.7031734726850573  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 30  Loss: 0.63811804647612  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.6671070437529124  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.6205195939628538  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.6294214964326329  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6562089606923109  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6805176802780579  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6536180587218496  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6716620330607839  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6824959144572403  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6501 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.6819446682929993  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6486272215843201  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6234878301620483  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6369 Acc: 0.6514777348499106\n",
      "Epoch 7/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6695129210826053  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.7067016008284781  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6343090653074341  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.66662247919337  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.6151018862546153  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.625093589500375  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6547587112036126  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6815748415344842  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6520372014491158  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6718836748767671  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6837676777413435  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6479 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.6831671595573425  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6467691659927368  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6193053126335144  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6349 Acc: 0.6514777348499106\n",
      "Epoch 8/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6696171163051986  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.7101085484146097  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6314237683428019  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.666568614489961  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.6108771766540485  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.6217987631199093  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6538732805674954  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6827666932083503  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6510340856888888  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6723604904580043  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6850955621793673  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6463 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.6844412088394165  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6455268859863281  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.61616450548172  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6336 Acc: 0.6514777348499106\n",
      "Epoch 9/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6699713706009789  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.713169283247862  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6292995559150768  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.6667705026311979  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.6075981818246874  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.619273222104266  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6533552995333594  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6839923752443156  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6503985372001353  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6729712139374442  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6864236705392613  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6453 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.685719907283783  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6446631550788879  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6136842370033264  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6326 Acc: 0.6514777348499106\n",
      "Epoch 10/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6704593689260946  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.7159358452465312  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6276872960281707  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.667109516759092  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.6050393742889355  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.6173479341770417  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6530838468570437  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6851424326328015  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6500244969233335  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6736048950976965  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6876309817778747  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6446 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.6868795156478882  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6440955996513367  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6118133664131165  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6319 Acc: 0.6514777348499106\n",
      "Epoch 11/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6709802980640234  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.7183019601022806  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6264933331399334  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.6675019748825435  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n",
      "Current step: 50  Loss: 0.6030430825105213  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.6158627140523094  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6529628592816152  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6862026312373928  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6498097524291098  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.6742309189128264  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6887376432103338  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6441 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.6879445910453796  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6437065005302429  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6103271245956421  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6315 Acc: 0.6514777348499106\n",
      "Epoch 12/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6715019588547875  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.7203589276920411  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6255817301369007  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n",
      "Current step: 40  Loss: 0.667901712920002  Acc: 0.6148048639297485  AllocMem (Mb): 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 50  Loss: 0.6014870281376716  Acc: 0.7300140261650085  AllocMem (Mb): 0.0\n",
      "Current step: 60  Loss: 0.6147271456977705  Acc: 0.706891655921936  AllocMem (Mb): 0.0\n",
      "Current step: 70  Loss: 0.6529342320391152  Acc: 0.640838623046875  AllocMem (Mb): 0.0\n",
      "Current step: 80  Loss: 0.6871248438205839  Acc: 0.5819435119628906  AllocMem (Mb): 0.0\n",
      "Current step: 90  Loss: 0.6497015093095342  Acc: 0.6464012265205383  AllocMem (Mb): 0.0\n",
      "Current step: 100  Loss: 0.674795587146491  Acc: 0.6032752990722656  AllocMem (Mb): 0.0\n",
      "Current step: 110  Loss: 0.6896830193748429  Acc: 0.5777999758720398  AllocMem (Mb): 0.0\n",
      "train Loss: 0.6438 Acc: 0.6566556917519129\n",
      "Current step: 10  Loss: 0.6888530254364014  Acc: 0.5792831182479858  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.6434537172317505  Acc: 0.6570907831192017  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6091980338096619  Acc: 0.7157996892929077  AllocMem (Mb): 0.0\n",
      "valid Loss: 0.6312 Acc: 0.6514777348499106\n",
      "Epoch 13/19\n",
      "----------\n",
      "Current step: 10  Loss: 0.6719774585060805  Acc: 0.608294665813446  AllocMem (Mb): 0.0\n",
      "Current step: 20  Loss: 0.7220694034160715  Acc: 0.5226997137069702  AllocMem (Mb): 0.0\n",
      "Current step: 30  Loss: 0.6248981235135034  Acc: 0.6886879205703735  AllocMem (Mb): 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28292\\1825415132.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28292\\1563067928.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs)\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[1;31m# the backward pass frees the graph memory, so there is no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                     \u001b[1;31m# need for torch.no_grad in this training pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                     \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss, accuracy = train(model, train_loader, valid_loader, loss_fn, opt, acc_fn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1808d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
