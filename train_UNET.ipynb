{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b98e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from hyperparameters import *\n",
    "from unet import UNet\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "203d6907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cafddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595 198 199\n"
     ]
    }
   ],
   "source": [
    "print(train_set_size, valid_set_size, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6bab737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1024, 3])\n",
      "torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_set:\n",
    "    print(x.size())\n",
    "    print(y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b373316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out: torch.Size([3, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels=3,\n",
    "            out_channels=1,\n",
    "            n_blocks=4,\n",
    "            start_filters=32,\n",
    "            activation='relu',\n",
    "            normalization='batch',\n",
    "            conv_mode='same',\n",
    "            dim=2)\n",
    "x = torch.randn(size=(3, 3, 1024,1024), dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "    \n",
    "print(f'Out: {out.shape}')\n",
    "\n",
    "# outpushape: [batch, channel, H, W]\n",
    "# inputshape: [batch, channel, 1024, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "833bc72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "n_iters = int(train_set_size / batch_size)\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "iterations = epochs * n_iters\n",
    "step_size = 2*n_iters\n",
    "model_name =\"{}epochs_lr{}_step{}\".format(epochs, lr, step_size)\n",
    "save_PATH = './model_name'\n",
    "if not os.path.exists(save_PATH):\n",
    "    os.mkdir(save_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad493e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da536e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_to_mask(outpus_squeezed):\n",
    "    #outpus shape [batch,512,512]\n",
    "    # sigmoid =nn.Sigmoid()\n",
    "    # mask = sigmoid(outpus_squeezed)\n",
    "    mask[mask<=0.5] = 0\n",
    "    mask[mask>0.5] = 1\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "353cdaf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mout_to_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn [38], line 4\u001b[0m, in \u001b[0;36mout_to_mask\u001b[0;34m(outpus_squeezed)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mout_to_mask\u001b[39m(outpus_squeezed):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#outpus shape [batch,512,512]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# sigmoid =nn.Sigmoid()\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m(outpus_squeezed)\n\u001b[1;32m      5\u001b[0m     mask[mask\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m     mask[mask\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.5\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sigmoid' is not defined"
     ]
    }
   ],
   "source": [
    "print(out_to_mask(out).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dccb662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_fn(outputs, y, batch_size):\n",
    "    '''\n",
    "    outputs: [batch, 1 , H, W]\n",
    "    y: [batch, H, W]\n",
    "    '''\n",
    "    H, W = y.shape\n",
    "    mask = out_to_mask(outputs)\n",
    "    num_pixels = H * W\n",
    "    \n",
    "    acc = torch.zeros([batch_size, 1])\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        equality_matrix = torch.eq(mask, y)\n",
    "        num_corr_pred = equality_matrix.sum()\n",
    "        acc_num = (num_corr_pred/num_pixels).item()\n",
    "        acc[i] = acc_num\n",
    "    acc_avg = acc.sum()/batch_size\n",
    "    return acc_avg.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86021155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(np.array([[1,0],[0,0]]))\n",
    "b = torch.tensor(np.array([[1,1],[0,0]]))\n",
    "accuracy = acc_fn(a,b,1)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d635d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=1):\n",
    "    start = time.time()\n",
    "    # model.cuda()\n",
    "\n",
    "    train_loss, valid_loss = [], []\n",
    "    accuracy = []\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "            step = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for x, y in dataloader:\n",
    "                # x = x.cuda()\n",
    "                # y = y.cuda()\n",
    "                x = torch.permute(x, (0, 3, 2, 1))\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                if phase == 'train':\n",
    "                    # zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x) # outputs: [batch,1,512,512]\n",
    "                    #print(outputs.shape)\n",
    "                    #outputs = torch.permute(outputs, (0,2,3,1)) #true y: [512,512,3]  outputs: [batch,512,512,3]\n",
    "                    #batch, h, w, channel = outputs.shape\n",
    "                    #outputs_loss = outputs.reshape(batch, h*w, channel)\n",
    "                    outputs = torch.squeeze(outputs)\n",
    "                    #y = y.reshape(batch, h*w, channel)\n",
    "                    # outputs = torch.permute(outputs, (0, 2, 3, 1)).contiguous().view(-1, inp.size(1))\n",
    "                    \n",
    "                    # y = torch.permute(y, (0,3,2,1))\n",
    "                    y = y.to(torch.float64)\n",
    "                    #print(y.shape) #y = [batch, 262144 (w*h)]\n",
    "                    #outputs_loss = outputs_loss.to(torch.float64)\n",
    "                    #print(outputs_loss.shape)\n",
    "                    \n",
    "                    #print('outputs', outputs, outputs.shape)\n",
    "                    #plt.imshow(outputs[0].detach().numpy())\n",
    "                    \n",
    "                    loss = loss_fn(outputs, y)\n",
    "\n",
    "                    # the backward pass frees the graph memory, so there is no \n",
    "                    # need for torch.no_grad in this training pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # scheduler.step()\n",
    "\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        outputs = torch.squeeze(outputs)\n",
    "                        loss = loss_fn(outputs, y)\n",
    "\n",
    "                # stats - whatever is the phase\n",
    "                acc = acc_fn(outputs, y)\n",
    "\n",
    "                running_acc  += acc*dataloader.batch_size\n",
    "                running_loss += loss*dataloader.batch_size \n",
    "\n",
    "                if step % 10 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))\n",
    "                    # print(torch.cuda.memory_summary())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_acc / len(dataloader.dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            train_loss.append(epoch_loss), accuracy.append(epoch_acc) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "        \n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))   \n",
    "            \n",
    "    return train_loss, valid_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9312e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "c = np.array([7,8,9])\n",
    "f = open(\"results/training_measures.csv\", \"w\")\n",
    "f.write(\"{},{},{}\\n\".format(\"Train Loss\", \"Valid Loss\",\"Train Acc\"))\n",
    "for x in zip(a,b,c):\n",
    "    f.write(\"{},{},{}\\n\".format(x[0], x[1], x[2]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e39ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss, valid_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [15], line 60\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, y)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# the backward pass frees the graph memory, so there is no \u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# need for torch.no_grad in this training pass\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deep_learning_ex_1/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deep_learning_ex_1/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss, accuracy = train(model, train_loader, valid_loader, loss_fn, opt, acc_fn, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e12d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"results/training_measures.csv\", \"w\")\n",
    "f.write(\"{},{},{}\\n\".format(\"Train Loss\", \"Valid Loss\",\"Train Acc\"))\n",
    "for x in zip(train_loss, valid_loss, accuracy):\n",
    "    f.write(\"{},{},{}\\n\".format(x[0], x[1], x[2]))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
